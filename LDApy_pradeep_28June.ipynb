{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################################\n",
    "################ LDA ################\n",
    "#####################################\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "#### import data###\n",
    "\n",
    "kdesc= pd.read_excel('kaggle_desc.xlsx')\n",
    "kd = kdesc[['Competition Id','Overview']].set_index('Competition Id')\n",
    "\n",
    "#doc_complete = kd[['Overview']]\n",
    "kdesc.to_pickle('kagdesc.pkl')\n",
    "kdesc = pd.read_pickle('kagdesc.pkl')\n",
    "\n",
    "kd.to_csv('kaggdescr.csv')\n",
    "\n",
    "### preprocess and vectorize data####\n",
    "### gensim tutorial\n",
    "\n",
    "docs = kdesc[['Overview']]\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "## initialize pattern\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## convert to lower\n",
    "docs['Overview'] = docs['Overview'].str.lower()\n",
    "## convert to list\n",
    "docs = docs['Overview'].tolist()\n",
    "## tokenize##\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx]= tokenizer.tokenize(docs[idx])\n",
    "\n",
    "## remove numbers, but not words that contain numbers\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "## remove words that are only one character\n",
    "docs = [[token for token in doc if len(token)>1] for doc in docs]\n",
    "\n",
    "## lemmatize words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "## remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[token for token in doc if token not in stop] for doc in docs]\n",
    "\n",
    "## compute bigrams\n",
    "from gensim.models import Phrases\n",
    "\n",
    "## add bigrams and trigrams to docs (only those that appear 3 times or more).\n",
    "bigram = Phrases(docs, min_count = 10) # ran three times with min count 3, 5, 10\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            #Token is a bigram, add to document\n",
    "            docs[idx].append(token)\n",
    "\n",
    "pd.DataFrame(docs).to_pickle('kaggle_topicsraw.pkl')\n",
    "\n",
    "## remove rare and common tokens\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "## create a dictionary representation of the docs\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "### rough work\n",
    "\n",
    "dictionary.filter_extremes\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "## filter out words that occur in less than 1 documents or more than 40% of the documents\n",
    "dictionary.filter_extremes(no_below = 1,no_above = 0.4)\n",
    "\n",
    "#Vectorize data\n",
    "## Bag of words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "## Bag of words to TfIdf \n",
    "\n",
    "\n",
    "print('no. of unique tokens: %d' % len(dictionary) )\n",
    "print('no of documents: %s' % len(corpus))\n",
    "\n",
    "\n",
    "### train the LDA model###\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "## set training parameters\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 300\n",
    "passes = 30\n",
    "iterations = 500\n",
    "eval_every = None \n",
    "\n",
    "## Make a index to word dictionary\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus = corpus, id2word = id2word, chunksize = chunksize, \\\n",
    "                       alpha = 'auto', eta ='auto', iterations = iterations,\\\n",
    "                       num_topics = num_topics,passes = passes, eval_every = eval_every)\n",
    "\n",
    "\n",
    "top_topics = model.top_topics(corpus, topn = 20)\n",
    "\n",
    "#Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics\n",
    " \n",
    "avg_topic_coherence = sum([t[1] for t in top_topics])/ num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence )\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "\n",
    "\n",
    "### import dataset###\n",
    "\n",
    "kdesc= pd.read_excel('kaggle_desc.xlsx')\n",
    "kdesc.head()\n",
    "\n",
    "kdesc.to_pickle('kagdesc.pkl')\n",
    "kdesc = pd.read_pickle('kagdesc.pkl')\n",
    "\n",
    "kd = kdesc[['Competition Id','Overview']]\n",
    "\n",
    "kd.to_csv('kaggdescr.csv')\n",
    "\n",
    "doc_complete = kd[['Overview']]\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete['Overview']]\n",
    "\n",
    "\n",
    "\n",
    "##### try 1#####\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "vec = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC, ngram_range = (1,2))\n",
    "doc_ngram = vec.fit_transform(doc_complete['Overview'])\n",
    "\n",
    "\n",
    "##### try 2###\n",
    "\n",
    "import nltk\n",
    "\n",
    "t1 = nltk.word_tokenize(doc_complete)\n",
    "\n",
    "########## try 3 #############\n",
    "\n",
    "import gensim\n",
    "\n",
    "### create a set of stopwords##\n",
    "stoplist = set('a for the and to in'.split(' '))\n",
    "\n",
    "dtm = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## part 2 creating dtm########\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#creating term dictionary of the corpus, where every unique term is assigned to an index\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#converting list of documents(corpus) into Document Term Matrix using dictionary prepared above\n",
    "\n",
    "dtm = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "\n",
    "####### part 3 LDA ############3\n",
    "\n",
    "\n",
    "#creating the object for LDA model using gensim library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# running and training LDA model on the document term matrix\n",
    "\n",
    "ldamodel = Lda(dtm, num_topics = 10, id2word = dictionary, passes = 50)\n",
    "\n",
    "\n",
    "##### results #####\n",
    "\n",
    "print(ldamodel.print_topics(num_topics = 10, num_words = 15))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "################ archive do not touch#############\n",
    "##################################################\n",
    "\n",
    "\n",
    "### import dataset###\n",
    "\n",
    "kdesc= pd.read_excel('kaggle_desc.xlsx')\n",
    "kdesc.head()\n",
    "\n",
    "kdesc.to_pickle('kagdesc.pkl')\n",
    "kdesc = pd.read_pickle('kagdesc.pkl')\n",
    "\n",
    "kd = kdesc[['Competition Id','Overview']]\n",
    "\n",
    "kd.to_csv('kaggdescr.csv')\n",
    "\n",
    "doc_complete = kd[['Overview']]\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete['Overview']]\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#creating term dictionary of the corpus, where every unique term is assigned to an index\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#converting list of documents(corpus) into Document Term Matrix using dictionary prepared above\n",
    "\n",
    "dtm = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "\n",
    "####### part 3 LDA ############3\n",
    "\n",
    "\n",
    "#creating the object for LDA model using gensim library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# running and training LDA model on the document term matrix\n",
    "\n",
    "ldamodel = Lda(dtm, num_topics = 10, id2word = dictionary, passes = 50)\n",
    "\n",
    "\n",
    "##### results #####\n",
    "\n",
    "print(ldamodel.print_topics(num_topics = 10, num_words = 15))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
